na.action(na.omit(c(1, NA))),
type="response")
#Train the model
gbm <- train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action = na.pass,
type="response")
gbm
gbm <- train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action = na.pass,
type="response")
#Train the model
gbm <- train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.omit(bike) ,
type="response")
train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.omit(bike) ,
type="response")
#Train the model
gbm <- train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.omit(count) ,
type="response")
train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.omit(count) ,
type="response")
#Train the model
gbm <- train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action(count) ,
type="response")
train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action(count) ,
type="response")
train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action() ,
type="response")
train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action=na.exclude,
type="response")
bike
#Train the model
gbm <- train(log_count~ target_season+holiday+workingday+target_weather+atemp+humidity+year+target_hour+weekday,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action=na.exclude,
type="response")
gbm
gbm$bestTune
gbm$bestTune
#Gather predictions
preds <- predict(gbm, newdata = test)
preds <- predict(gbm, newdata = test)
predict(gbm, newdata = test)
bike %>% filter(is.na(count))
test <- bike %>% filter(is.na(count))
preds <- predict(gbm, newdata = test)
predict(gbm, newdata = test)
preds <- expm1(preds)
preds.frame <- data.frame(datetime = test$datetime, count = preds)
write.csv(preds.frame, "BikeSharingfinalsubmission", row.names = FALSE)
#Train the model
gbm <- train(log_count~ .,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action=na.exclude,
type="response")
summary(bike)
#Train the model
gbm <- train(log_count~ target_season + target_month + target_weather + target_year + target_hour + weekday,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action=na.exclude,
type="response")
#Gather predictions
preds <- predict(gbm, newdata = test)
#Change preds back from log
preds <- expm1(preds)
submission <- data.frame(datetime = bike %>% filter (id =="test") %>% pull(datetime),
count = preds)
write.csv(x = submission, file = "./BikeSharingfinalsubmission.csv", row.names = FALSE)
bike$hour <- factor(hour(bike$datetime))
bike$month <- factor(month(bike$datetime))
bike$holiday
bike
bike$season <- factor(bike$season)
bike$weather <- factor(bike$weather)
bike$hour <- factor(hour(bike$datetime))
bike$month <- factor(month(bike$datetime))
bike$times <-  as.POSIXct(strftime(ymd_hms(bike$datetime), format="%H:%M:%S"), format="%H:%M:%S")
bike$weekday <- wday(ymd_hms(bike$datetime), label = TRUE)
bike$holiday <- as.factor(bike$holiday)
bike$workingday <- as.factor(bike$workingday)
bike$log_count <- log10(bike$count) #Change to log because the metric for the competition is rmsle and also the distribution of counts is skewed.
##Libraries I need
library(tidyverse)
library(DataExplorer)
library(caret)
library(vroom)
library(lubridate)
## Read in the data
bike.train <- vroom("../Bikesharing/train.csv")
bike.test <- vroom("../Bikesharing/test.csv")
bike <- bind_rows(train = bike.train, test = bike.test, .id = "id")
## Drop casual and registered
bike <- bike %>% select(-casual, -registered)
bike$season <- factor(bike$season)
bike$weather <- factor(bike$weather)
bike$hour <- factor(hour(bike$datetime))
bike$month <- factor(month(bike$datetime))
bike$times <-  as.POSIXct(strftime(ymd_hms(bike$datetime), format="%H:%M:%S"), format="%H:%M:%S")
bike$weekday <- wday(ymd_hms(bike$datetime), label = TRUE)
bike$holiday <- as.factor(bike$holiday)
bike$workingday <- as.factor(bike$workingday)
bike$log_count <- log10(bike$count) #Change to log because the metric for the competition is rmsle and also the distribution of counts is skewed.
bike$log_count
ggplot(bike) + geom_histogram(mapping = aes(x=count), bins = 20, fill = 'gray', col = 'black') ##dist of count
aggregate(bike[bike$id == 'train',] %>% select(count),list(bike[bike$id == 'train',]$day), mean ) ##mean count by day
ggplot(data=bike, aes(x= times, y=count, color = as.factor(season))) + geom_point() #count by time with season group
ggplot(data = bike[bike$id == 'train',], aes(x=datetime, y=count, color=as.factor(season))) +geom_point() ##season
ggplot(data = bike, aes(x=hour(datetime), y=count, color=as.factor(hour(datetime)))) + geom_point() ##hour
ggplot(bike, aes(x=weekday, y=count)) + geom_boxplot() ##count boxplot by weekday
ggplot(bike, aes(x=weather, y=count)) + geom_boxplot() ## count boxplot by weather
ggplot(data = bike, aes(x= bike$month, y= count)) + geom_point() ## count by month
ggplot(data = bike, aes(x= month, y= count)) + geom_point() ## count by month
bike <- bike %>% select(-atemp)
bike
#Split test/train
train <- bike %>% filter(!is.na(count))
test <- bike %>% filter(is.na(count))
train
#Grid space to search for the best hyperparameters
xgbGrid <- expand.grid(nrounds = c(100,200),  # this is n_estimators in the python code above
max_depth = c(10, 15, 20, 25, 30),
colsample_bytree = seq(0.5, 0.9, length.out = 5),
## The values below are default values in the sklearn-api.
eta = 0.1,
gamma=0,
min_child_weight = 1,
subsample = 1
)
xgb_trcontrol = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE,
verboseIter = FALSE,
returnData = FALSE
)
#Train the model
gbm <- train(count~ -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action=na.exclude,
type="response")
#Train the model
gbm <- train(count~, -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action=na.exclude,
type="response")
gbm <- train(count~, -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action=na.exclude,
type="response")
gbm <- train(count~. -datetime,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action=na.exclude,
type="response")
summary(bike)
#Train the model
gbm <- train(log_count~ season + holiday + workingday + hour + month + weekday + times,
data = bike,
trControl = xgb_trcontrol,
tuneGrid = xgbGrid,
method = "xgbTree",
verbose = FALSE,
na.action=na.exclude,
type="response")
gbm$bestTune
#Gather predictions
preds <- predict(gbm, newdata = test)
#Change preds back from log
preds <- expm1(preds)
preds <- predict(gbm, newdata = bike %>% filter(id =="test"))
submission <- data.frame(datetime = bike %>% filter (id =="test") %>% pull(datetime),
count = preds)
write.csv(x = submission, file = "./BikeSharingfinalsubmission.csv", row.names = FALSE)
write.csv(x = submission, file = "./BikeSharingfinalsubmission.csv", row.names = FALSE)
#Gather predictions
preds <- predict(gbm, newdata = test)
#Change preds back from log
preds <- expm1(preds)
preds <- predict(gbm, newdata = bike %>% filter(id =="test"))
submission <- data.frame(datetime = bike %>% filter (id =="test") %>% pull(datetime),
count = preds)
write.csv(x = submission, file = "./BikeSharingfinalsubmission.csv", row.names = FALSE)
#Grid space to search for the best hyperparameters
xgbGrid <- expand.grid(nrounds = c(100,200),  # this is n_estimators in the python code above
#Grid space to search for the best hyperparameters
xgbGrid <- expand.grid(nrounds = c(100,200),  # this is n_estimators in the python code above
max_depth = c(10, 15, 20, 25, 30),
colsample_bytree = seq(0.5, 0.9, length.out = 5),
## The values below are default values in the sklearn-api.
eta = 0.1,
gamma=0,
min_child_weight = 1,
subsample = 1)
bike$hour <- hour(bike$datetime) %>% as.factor()
bike$times <- as.POSIXct(strftime(ymd_hms(bike$datetime), format="%H:%M:%S"), format="%H:%M:%S")
bike$season <- as.factor(bike$season)
## Exploratory Plots
ggplot(data=bike, aes(x= times, y=count, color = as.factor(season))) +
geom_point()
## Target encoding (popular one)
bike$times <- lm(count~times, data = bike) %>%
predict(., newdata = bike %>% select(-count))
bike$hour <- lm(count~hour, data = bike)%>%
predict(., newdata = bike %>% select(-count))
#Grid space to search for the best hyperparameters
xgbGrid <- expand.grid(nrounds = c(100,200),  # this is n_estimators in the python code above
max_depth = c(10, 15, 20, 25, 30),
colsample_bytree = seq(0.5, 0.9, length.out = 5),
## The values below are default values in the sklearn-api.
eta = 0.1,
gamma=0,
min_child_weight = 1,
subsample = 1)
bike.model <- train(form = count~ times + hour + season + temp,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = xgb_trcontrol,
tuneGrid =xgbGrid,
method = "xgbTree",
verbose = FALSE,
type = "response")
bike.model <- train(form = count~ times + hour + season + temp,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = xgb_trcontrol,
tuneGrid =xgbGrid,
method = "xgbTree",
verbose = FALSE,
type = "response")
bike.model <- train(form = count~ .,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = xgb_trcontrol,
tuneGrid =xgbGrid,
method = "xgbTree",
verbose = FALSE,
type = "response")
bike.model <- train(form = count~ . -datetime,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = xgb_trcontrol,
tuneGrid =xgbGrid,
method = "xgbTree",
verbose = FALSE,
type = "response")
bike.model <- train(form = count~. -datetime,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = xgb_trcontrol,
tuneGrid =xgbGrid,
method = "xgbTree",
verbose = FALSE,
type = "response")
bike$hour <- hour(bike$datetime) %>% as.factor()
bike$times <- as.POSIXct(strftime(ymd_hms(bike$datetime), format="%H:%M:%S"), format="%H:%M:%S")
bike$season <- as.factor(bike$season)
bike$times <- lm(count~times, data = bike) %>%
predict(., newdata = bike %>% select(-count))
bike$hour <- lm(count~hour, data = bike)%>%
predict(., newdata = bike %>% select(-count))
#Grid space to search for the best hyperparameters
xgbGrid <- expand.grid(nrounds = c(100,200),  # this is n_estimators in the python code above
max_depth = c(10, 15, 20, 25, 30),
colsample_bytree = seq(0.5, 0.9, length.out = 5),
## The values below are default values in the sklearn-api.
eta = 0.1,
gamma=0,
min_child_weight = 1,
subsample = 1)
bike.model <- train(form = count~. -datetime,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = xgb_trcontrol,
tuneGrid =xgbGrid,
method = "xgbTree",
verbose = FALSE,
type = "response")
bike
bike.model <- train(form = log_count~ hour + times + month + season + temp + weekday ,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = xgb_trcontrol,
tuneGrid =xgbGrid,
method = "xgbTree",
verbose = FALSE,
type = "response")
bike.train <- vroom("../Bikesharing/train.csv")
bike.test <- vroom("../Bikesharing/test.csv")
bike <- bind_rows(train = bike.train, test = bike.test, .id = "id")
# combine train and test dataset and then create first column name id (bind_rows :tidyverse)
# use "bike %>% filter(id == "train")" to filter bike.train data
## Drop casual and registered
bike <- bike %>% select(-casual, -registered)
## Feature Engineering
bike$hour <- hour(bike$datetime) %>% as.factor()
bike$times <- as.POSIXct(strftime(ymd_hms(bike$datetime), format="%H:%M:%S"), format="%H:%M:%S")
bike$season <- as.factor(bike$season)
## Exploratory Plots
ggplot(data=bike, aes(x= times, y=count, color = as.factor(season))) +
geom_point()
## Target encoding (popular one)
bike$times <- lm(count~times, data = bike) %>%
predict(., newdata = bike %>% select(-count))
bike.model <- train(form = count~ times + holiday + temp,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = trainControl(
method = "boot",
number = 10, #how many sample
repeats = 2))
plot(bike.model)
preds <- predict(bike.model, newdata = bike %>% filter(id =="test"))
submission <- data.frame(datetime = bike %>% filter (id =="test") %>% pull(datetime),
count = preds)
write.csv(x = submission, file = "./MyFifthSubmission.csv", row.names = FALSE)
bike$times <- lm(count~times, data = bike) %>%
predict(., newdata = bike %>% select(-count))
bike$hour <- lm(count~hour, data = bike) %>%
predict(., newdata = bike %>% select(-count))
bike$log_count <- log10(bike$count)
bike$times <- lm(log_count~times, data = bike) %>%
predict(., newdata = bike %>% select(-count))
bike$hour <- lm(log_count~hour, data = bike) %>%
predict(., newdata = bike %>% select(-count))
bike.model <- train(form = log_count~ times + hour + holiday + temp,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = trainControl(
method = "boot",
number = 10 ))
plot(bike.model)
preds <- predict(bike.model, newdata = bike %>% filter(id =="test"))
submission <- data.frame(datetime = bike %>% filter (id =="test") %>% pull(datetime),
count = 10^preds)
write.csv(x = submission, file = "./MyFifthSubmission.csv", row.names = FALSE)
bike.model <- train(form = log_count~ .,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = trainControl(
method = "boot",
number = 10 ))#how many sample
#Grid space to search for the best hyperparameters
xgbGrid <- expand.grid(nrounds = c(100,200),  # this is n_estimators in the python code above
max_depth = c(10, 15, 20, 25, 30),
colsample_bytree = seq(0.5, 0.9, length.out = 5),
## The values below are default values in the sklearn-api.
eta = 0.1,
gamma=0,
min_child_weight = 1,
subsample = 1
)
bike.model <- train(form = log_count~ times + hour + holiday + temp,
data = bike %>% filter (id =='train'),
method = "xgbTree", #regression of the assumption
verbose = FALSE,
tuneGrid = xgbGrid,
type="response", #how its evaulating prediction
trControl = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE,
verboseIter = FALSE,
returnData = FALSE))#how many sample
plot(bike.model)
preds <- predict(bike.model, newdata = bike %>% filter(id =="test"))
submission <- data.frame(datetime = bike %>% filter (id =="test") %>% pull(datetime),
count = 10^preds)
write.csv(x = submission, file = "./MyFifthSubmission.csv", row.names =
write.csv(x = submission, file = "./MyFifthSubmission.csv", row.names = FALSE)
write.csv(x = submission, file = "./MyFifthSubmission.csv", row.names = FALSE)
write.csv(x = submission, file = "./MyFifthSubmission.csv", row.names = FALSE)
bike.model <- train(form = log_count~ times + hour + holiday + temp,
data = bike %>% filter (id =='train'),
method = "rf", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = trainControl(
method = "boot",
number = 10 ))#how many sample
plot(bike.model)
preds <- predict(bike.model, newdata = bike %>% filter(id =="test"))
submission <- data.frame(datetime = bike %>% filter (id =="test") %>% pull(datetime),
count = 10^preds)
write.csv(x = submission, file = "./MyFifthSubmission.csv", row.names = FALSE)
bike.model <- train(form = log_count~ times + hour + holiday + temp,
data = bike %>% filter (id =='train'),
method = "ranger", #regression of the assumption
tuneLength = 5, #how its evaulating prediction
trControl = trainControl(
method = "repeatedcv",
number = 10,
repeats = 2))#how many sample
plot(bike.model)
preds <- predict(bike.model, newdata = bike %>% filter(id =="test"))
submission <- data.frame(datetime = bike %>% filter (id =="test") %>% pull(datetime),
count = 10^preds)
write.csv(x = submission, file = "./MyFifthSubmission.csv", row.names = FALSE)
??repeatedcv
?train
ggplot(bike) + geom_histogram(mapping = aes(x=count),
bins = 20, fill = 'gray',
col = 'black') ##dist of count
ggplot(bike) + geom_histogram(mapping = aes(x=count),
bins = 20, fill = 'gray',
col = 'black') ##dist of count
ggplot(bike) +
geom_histogram(mapping = aes(x=count), bins = 20,
fill = 'gray', col = 'black') ##dist of count
ggplot(bike) +
geom_histogram(mapping = aes(x=count),
fill = 'gray', col = 'black') ##dist of count
ggplot(bike) +
geom_histogram(mapping = aes(x=count), bins= 30,
fill = 'gray', col = 'black') ##dist of count
## Exploratory Plots
ggplot(data=bike, aes(x= times, y=count, color = as.factor(season))) +
geom_point()
ggplot(data=bike, aes(x= times, y=count, color = as.factor(season))) +
geom_point()
