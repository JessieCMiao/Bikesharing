##Libraries I need 
library(tidyverse)
library(DataExplorer)
library(caret)
library(vroom)
library(lubridate)



## Read in the data 
bike.train <- vroom("../Bikesharing/train.csv")
bike.test <- vroom("../Bikesharing/test.csv")
bike <- bind_rows(train = bike.train, test = bike.test, .id = "id")

# combine train and test dataset and then create first column name id (bind_rows :tidyverse)
# use "bike %>% filter(id == "train")" to filter bike.train data

## Drop casual and registered
bike <- bike %>% select(-casual, -registered)

## Feature Engineering 
bike$season <- factor(bike$season)
bike$weather <- factor(bike$weather)
bike$hour <- factor(hour(bike$datetime))
bike$month <- factor(month(bike$datetime))
bike$times <-  as.POSIXct(strftime(ymd_hms(bike$datetime), format="%H:%M:%S"), format="%H:%M:%S")
bike$weekday <- wday(ymd_hms(bike$datetime), label = TRUE)
bike$holiday <- as.factor(bike$holiday)
bike$workingday <- as.factor(bike$workingday)
bike$log_count <- log10(bike$count) #Change to log because the metric for the competition is rmsle and also the distribution of counts is skewed.


## Exploratory Plots 
ggplot(bike) + geom_histogram(mapping = aes(x=count), bins = 20, fill = 'gray', col = 'black') ##dist of count
aggregate(bike[bike$id == 'train',] %>% select(count),list(bike[bike$id == 'train',]$day), mean ) ##mean count by day
ggplot(data=bike, aes(x= times, y=count, color = as.factor(season))) + geom_point() #count by time with season group
ggplot(data = bike[bike$id == 'train',], aes(x=datetime, y=count, color=as.factor(season))) +geom_point() ##season
ggplot(data = bike, aes(x=hour(datetime), y=count, color=as.factor(hour(datetime)))) + geom_point() ##hour
ggplot(bike, aes(x=weekday, y=count)) + geom_boxplot() ##count boxplot by weekday
ggplot(bike, aes(x=weather, y=count)) + geom_boxplot() ## count boxplot by weather
ggplot(data = bike, aes(x= month, y= count)) + geom_point() ## count by month 

bike <- bike %>% select(-atemp)



#Split test/train
train <- bike %>% filter(!is.na(count))
test <- bike %>% filter(is.na(count))


#Grid space to search for the best hyperparameters
xgbGrid <- expand.grid(nrounds = c(100,200),  # this is n_estimators in the python code above
                       max_depth = c(10, 15, 20, 25, 30),
                       colsample_bytree = seq(0.5, 0.9, length.out = 5),
                       ## The values below are default values in the sklearn-api. 
                       eta = 0.1,
                       gamma=0,
                       min_child_weight = 1,
                       subsample = 1
)


#Specify cross-validation method and number of folds. Also enable parallel computation

xgb_trcontrol = trainControl(
  method = "cv",
  number = 5,  
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)

summary(bike)

#Train the model
gbm <- train(log_count~ season + holiday + workingday + hour + month + weekday + times,
             data = bike, 
             trControl = xgb_trcontrol,
             tuneGrid = xgbGrid,
             method = "xgbTree",
             verbose = FALSE,
             na.action=na.exclude,
             type="response")
gbm$bestTune


#Gather predictions
preds <- predict(gbm, newdata = test)

#Change preds back from log
preds <- expm1(preds)

preds <- predict(gbm, newdata = bike %>% filter(id =="test"))
submission <- data.frame(datetime = bike %>% filter (id =="test") %>% pull(datetime),
                         count = preds)

write.csv(x = submission, file = "./BikeSharingfinalsubmission.csv", row.names = FALSE)

